# k8s-tools: 
#   Pin or customize versions for your whole k8s toolchain in one place.
#
#   Docs: https://github.com/elo-enterprises/k8s-tools
#   Latest: https://github.com/elo-enterprises/k8s-tools/tree/master/k8s-tools.yml
services:
  k8s: &base
    image: k8s:base
    hostname: k8s-base
    environment:
      KUBECONFIG: "${KUBECONFIG}"
      DOCKER_UID: ${DOCKER_UID:-1000}
      DOCKER_GID: ${DOCKER_GID:-1000}
      DOCKER_UGNAME: ${DOCKER_UGNAME:-user}
      KREW_ROOT: /home/${DOCKER_UGNAME:-user}/.krew
      TERM: ${TERM:-xterm-256color}
    build:
      context: .
      dockerfile_inline: |
        ARG DOCKER_UID=${DOCKER_UID:-1000}
        ARG DOCKER_GID=${DOCKER_UGNAME:-user}
        ARG DOCKER_UGNAME=${DOCKER_UGNAME:-user}
        FROM ${ALPINE_K8S_VERSION:-alpine/k8s:1.30.0} as builder
        RUN apk --no-cache add procps make 
        RUN cp /krew-* /usr/bin/krew
        FROM ghcr.io/charmbracelet/gum as gum
        FROM ${ALPINE_K8S_VERSION:-alpine/k8s:1.30.0} 
        COPY --from=gum /usr/local/bin/gum /usr/bin
        COPY --from=builder /usr/bin/make /bin/ps /usr/bin/krew /bin
        RUN apk --no-cache add ncurses shadow
        RUN groupadd --gid ${DOCKER_GID:-1000} docker
        RUN useradd --uid ${DOCKER_UID:-1000} --create-home -g docker ${DOCKER_UGNAME:-user}
        USER ${DOCKER_UGNAME:-user}
        RUN KREW_ROOT=/home/${DOCKER_UGNAME:-user}/.krew krew install ctx ns sick-pods
        RUN KREW_ROOT=/home/${DOCKER_UGNAME:-user}/.krew krew install ${KREW_PLUGINS:-ktop}
        USER root 
        RUN cp /home/${DOCKER_UGNAME:-user}/.krew/bin/kubectl-ns /usr/bin/kubens
        RUN cp /home/${DOCKER_UGNAME:-user}/.krew/bin/kubectl-ctx /usr/bin/kubectx
        USER ${DOCKER_UGNAME:-user}
        ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/${DOCKER_UGNAME:-user}/.krew/bin
    user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    network_mode: host
    working_dir: /workspace
    volumes:
      # Share the docker sock.  Almost everything will need this
      - /var/run/docker.sock:/var/run/docker.sock
      # Share the working directory with containers, plus ~/.kube
      - ${PWD}:/workspace
      - ${HOME}/.kube:/home/${DOCKER_UGNAME:-user}/.kube
      # NB: `.cache` and `.config` as below are used by helm, maybe others?
      - ${HOME}/.cache:/home/${DOCKER_UGNAME:-user}/.cache
      - ${HOME}/.config/helm:/home/${DOCKER_UGNAME:-user}/.config/helm
      - ${HOME}/.local:/home/${DOCKER_UGNAME:-user}/.local:ro
      # NB: something like this if you're working with EKS and need AWS creds
      # - ${HOME}/.aws:/home/user/.aws
      # NB: something like this if you only want to share one file.
      # - "${KUBECONFIG}:/kubeconfig.conf:ro"
    tty: true 

  # https://helm.sh/docs/
  helm:
    <<: *base
    entrypoint: helm
  
  # https://kubernetes.io/docs/reference/kubectl/
  kubectl:
    <<: *base
    entrypoint: kubectl

  # https://github.com/kubernetes-sigs/kustomize
  kustomize:
    <<: *base
    entrypoint: kustomize

  # https://github.com/databus23/helm-diff
  helm-diff:
    <<: *base
    entrypoint: helm-diff

  # https://github.com/helm-unittest/helm-unittest
  helm-unittest:
    <<: *base
    entrypoint: helm-unittest

  # https://github.com/chartmuseum/helm-push
  helm-push:
    <<: *base
    entrypoint: helm-push

  # https://github.com/kubernetes-sigs/aws-iam-authenticator
  aws-iam-authenticator:
    <<: *base
    entrypoint: aws-iam-authenticator

  # https://github.com/weaveworks/eksctl
  eksctl:
    <<: *base
    entrypoint: eksctl

  # https://github.com/aws/aws-cli
  awscli: # v1
    <<: *base
    entrypoint: awscli

  # https://github.com/bitnami-labs/sealed-secrets
  kubeseal:
    <<: *base
    entrypoint: kubeseal

  # https://github.com/kubernetes-sigs/krew
  krew:
    <<: *base
    entrypoint: krew

  # https://github.com/helmfile/vals
  vals:
    <<: *base
    entrypoint: vals

  # https://github.com/yannh/kubeconform
  kubeconform:
    <<: *base
    entrypoint: kubeconform

  # https://knative.dev/docs/client/install-kn/
  kn: &knative
    <<: *base
    depends_on: ['k8s']
    image: k8s:kn
    build:
      context: .
      dockerfile_inline: |
        FROM k8s:base as base
        FROM ghcr.io/knative/func/func as builder
        FROM gcr.io/knative-releases/knative.dev/client/cmd/kn:${KN_CLI_VERSION:-v1.14.0}
        COPY --from=builder /ko-app/func /ko-app/func
        COPY --from=base /usr/bin/kubectl /usr/bin/
        RUN apk --no-cache add bash procps make
        RUN cp /ko-app/func /usr/bin/kn-func

  # https://github.com/arttor/helmify
  helmify:
    <<: *base
    depends_on: ['k8s']
    image: k8s:helmify
    build:
      context: .
      dockerfile_inline: |
        FROM debian
        RUN apt-get update && apt-get install -y curl
        RUN cd /tmp && curl -s -Lo helmify.tgz \
            https://github.com/arttor/helmify/releases/download/${HELMIFY_CLI_VERSION:-v0.4.12}/helmify_Linux_i386.tar.gz
        RUN cd /tmp && tar -zxvf helmify.tgz && chmod +x helmify && mv helmify /usr/local/bin/
    entrypoint: helmify
    tty: false
    stdin_open: true

  # https://fission.io/docs/installation/
  fission:
    <<: *base
    depends_on: ['k8s']
    image: k8s:fission
    build:
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root 
        RUN curl -s -Lo fission \
            https://github.com/fission/fission/releases/download/${FISSION_CLI_VERSION:-v1.20.1}/fission-${FISSION_CLI_VERSION:-v1.20.1}-linux-amd64
        RUN chmod +x fission && mv fission /usr/local/bin/
        USER ${DOCKER_UGNAME:-user}
    entrypoint: fission


  # https://github.com/kubernetes/kompose/blob/main/docs/installation.md#github-release
  kompose:
    <<: *base
    depends_on: ['k8s']
    image: k8s:kompose
    build:
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root 
        RUN curl -L https://github.com/kubernetes/kompose/releases/download/v1.33.0/kompose-linux-amd64 -o /usr/bin/kompose
        RUN chmod ugo+x /usr/bin/kompose
        USER ${DOCKER_UGNAME:-user}
    entrypoint: kompose
  
  # https://argo-workflows.readthedocs.io/en/latest/walk-through/argo-cli/
  # FIXME: pin version
  argo:
    <<: *base 
    depends_on: ['k8s']
    image: k8s:argo 
    build:
      context: . 
      dockerfile_inline: |
        FROM argoproj/argocli:${ARGO_CLI_VERSION:-v3.4.17} as argo
        FROM k8s:base 
        COPY --from=argo /bin/argo /bin/argo
    entrypoint: argo 
  
  # https://github.com/txn2/kubefwd
  # FIXME: pin version
  kubefwd:
    <<: *base 
    depends_on: ['k8s']
    image: k8s:kubefwd 
    user: root 
    build:
      context: . 
      dockerfile_inline: |
        FROM txn2/kubefwd as builder 
        FROM k8s:base
        COPY --from=builder /kubefwd /usr/bin/kubefwd
    entrypoint: kubefwd
    volumes: 
      # Same as the base volumes, plus /etc/hosts for kubefwd to sync DNS
      - /etc/hosts:/etc/hosts:rw
      - ${PWD}:/workspace
      - /var/run/docker.sock:/var/run/docker.sock
      - ${HOME}/.kube:/home/${DOCKER_UGNAME:-user}/.kube
      - ${HOME}/.cache:/home/${DOCKER_UGNAME:-user}/.cache
      - ${HOME}/.config/helm:/home/${DOCKER_UGNAME:-user}/.config/helm
      - ${HOME}/.local:/home/${DOCKER_UGNAME:-user}/.local:ro

  # https://k3d.io/
  k3d:
    <<: *base
    depends_on: ['k8s']
    image: k8s:k3d
    build:
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root
        RUN curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh \
          | TAG=${K3D_VERSION:-v5.6.3} bash
        USER ${DOCKER_UGNAME:-user}
    entrypoint: k3d
  
  # https://github.com/jesseduffield/lazydocker
  lazydocker:
    <<: *base
    depends_on: ['k8s']
    image: k8s:lazydocker
    build:
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root
        RUN wget https://github.com/jesseduffield/lazydocker/releases/download/v${LAZY_DOCKER_CLI_VERSION:-0.23.1}/lazydocker_${LAZY_DOCKER_CLI_VERSION:-0.23.1}_Linux_x86_64.tar.gz 
        RUN tar -zxvf lazydocker*
        RUN mv lazydocker /usr/bin && rm lazydocker*
        USER ${DOCKER_UGNAME:-user}
    entrypoint: lazydocker
  

  # https://github.com/kubernetes-sigs/kind
  kind: 
    image: 'k8s:kind'
    build:
      context: .
      dockerfile_inline: |
        FROM debian
        RUN apt-get update && apt-get install -y curl
        RUN [ $(uname -m) = x86_64 ] \
          && curl -Lo /usr/bin/kind https://kind.sigs.k8s.io/dl/${KIND_CLI_VERSION:-v0.23.0}/kind-$(uname)-amd64 \
          && chmod o+x /usr/bin/kind
    entrypoint: /usr/bin/kind

  # https://k9scli.io/
  k9s:
    # NB: no inheritance from `base` since `build` conflicts with `image`.
    build: https://github.com/derailed/k9s.git#master:/
    tty: true
    network_mode: host
    volumes:
      - type: bind
        source: ${KUBECONFIG}
        target: /kubeconfig.conf
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      KUBECONFIG: "/kubeconfig.conf"
    entrypoint: k9s

  yq:
    # NB: included in base, but this option is slimmer
    image: mikefarah/yq:4.43.1

  jq:
    # NB: included in base, but this option is slimmer
    image: ghcr.io/jqlang/jq:1.7.1
